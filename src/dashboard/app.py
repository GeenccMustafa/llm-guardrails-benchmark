"""Interactive Dashboard for Guardrails Benchmark Results."""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
from pathlib import Path
import glob

st.set_page_config(page_title="Guardrails Benchmark Dashboard", layout="wide", page_icon="üõ°Ô∏è")

# Custom CSS
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        color: #1f77b4;
        text-align: center;
        margin-bottom: 2rem;
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 0.5rem 0;
    }
    .success {color: #28a745;}
    .warning {color: #ffc107;}
    .danger {color: #dc3545;}
</style>
""", unsafe_allow_html=True)

@st.cache_data
def load_latest_results():
    """Load the most recent benchmark results."""
    results_dir = Path("src/results")
    csv_files = list(results_dir.glob("benchmark_results_*.csv"))
    json_files = list(results_dir.glob("benchmark_summary_*.json"))
    
    if not csv_files:
        return None, None
    
    latest_csv = max(csv_files, key=lambda x: x.stat().st_mtime)
    latest_json = max(json_files, key=lambda x: x.stat().st_mtime) if json_files else None
    
    df = pd.read_csv(latest_csv)
    summary = None
    if latest_json:
        with open(latest_json, 'r') as f:
            summary = json.load(f)
    
    return df, summary

def main():
    st.markdown('<h1 class="main-header">üõ°Ô∏è Guardrails Benchmark Dashboard</h1>', unsafe_allow_html=True)
    
    # Load data
    df, summary = load_latest_results()
    
    if df is None:
        st.error("‚ùå No benchmark results found. Run the benchmark first!")
        st.code("python src/benchmarks/run_benchmark.py")
        return
    
    # Sidebar
    st.sidebar.header("üìä Filters")
    
    models = st.sidebar.multiselect(
        "Select Models",
        options=df['model'].unique(),
        default=df['model'].unique()
    )
    
    guardrails = st.sidebar.multiselect(
        "Select Guardrails",
        options=df['guardrail'].unique(),
        default=df['guardrail'].unique()
    )
    
    categories = st.sidebar.multiselect(
        "Select Categories",
        options=df['category'].unique(),
        default=df['category'].unique()
    )
    
    # Filter data
    filtered_df = df[
        (df['model'].isin(models)) &
        (df['guardrail'].isin(guardrails)) &
        (df['category'].isin(categories))
    ]
    
    # Overview Metrics
    st.header("üìà Overview")
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Total Tests", len(filtered_df))
    with col2:
        success_rate = (filtered_df['success'].sum() / len(filtered_df) * 100) if len(filtered_df) > 0 else 0
        st.metric("Success Rate", f"{success_rate:.1f}%")
    with col3:
        input_block_rate = (filtered_df['input_blocked'].sum() / len(filtered_df) * 100) if len(filtered_df) > 0 else 0
        st.metric("Input Blocked", f"{input_block_rate:.1f}%")
    with col4:
        output_block_rate = (filtered_df['output_blocked'].sum() / len(filtered_df) * 100) if len(filtered_df) > 0 else 0
        st.metric("Output Blocked", f"{output_block_rate:.1f}%")
    
    # Tabs
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "üõ°Ô∏è Guardrail Performance",
        "‚ö° Model Comparison", 
        "üìä Category Analysis",
        "üîç Detailed Results",
        "üí° Recommendations"
    ])
    
    with tab1:
        st.subheader("Guardrail Effectiveness")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Input blocking by guardrail
            guard_input_stats = filtered_df.groupby('guardrail').agg({
                'input_blocked': 'sum',
                'guardrail': 'count'
            }).rename(columns={'guardrail': 'total'})
            guard_input_stats['block_rate'] = (guard_input_stats['input_blocked'] / guard_input_stats['total'] * 100)
            
            fig = px.bar(
                guard_input_stats.reset_index(),
                x='guardrail',
                y='block_rate',
                title="Input Block Rate by Guardrail",
                labels={'block_rate': 'Block Rate (%)', 'guardrail': 'Guardrail'},
                color='block_rate',
                color_continuous_scale='Reds'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Output blocking by guardrail
            guard_output_stats = filtered_df.groupby('guardrail').agg({
                'output_blocked': 'sum',
                'guardrail': 'count'
            }).rename(columns={'guardrail': 'total'})
            guard_output_stats['block_rate'] = (guard_output_stats['output_blocked'] / guard_output_stats['total'] * 100)
            
            fig = px.bar(
                guard_output_stats.reset_index(),
                x='guardrail',
                y='block_rate',
                title="Output Block Rate by Guardrail",
                labels={'block_rate': 'Block Rate (%)', 'guardrail': 'Guardrail'},
                color='block_rate',
                color_continuous_scale='Blues'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Processing time comparison
        st.subheader("‚è±Ô∏è Processing Time")
        
        time_data = filtered_df.groupby('guardrail').agg({
            'input_check_time_ms': 'mean',
            'output_check_time_ms': 'mean'
        }).reset_index()
        
        fig = go.Figure(data=[
            go.Bar(name='Input Check', x=time_data['guardrail'], y=time_data['input_check_time_ms']),
            go.Bar(name='Output Check', x=time_data['guardrail'], y=time_data['output_check_time_ms'])
        ])
        fig.update_layout(
            title="Average Processing Time by Guardrail",
            xaxis_title="Guardrail",
            yaxis_title="Time (ms)",
            barmode='group'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with tab2:
        st.subheader("Model Performance Comparison")
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Model response time
            model_stats = filtered_df.groupby('model').agg({
                'model_time_seconds': 'mean'
            }).reset_index()
            
            fig = px.bar(
                model_stats,
                x='model',
                y='model_time_seconds',
                title="Average Response Time by Model",
                labels={'model_time_seconds': 'Time (seconds)', 'model': 'Model'},
                color='model_time_seconds',
                color_continuous_scale='Viridis'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Total processing time (model + guardrails)
            total_time_stats = filtered_df.groupby('model').agg({
                'total_time_ms': 'mean'
            }).reset_index()
            total_time_stats['total_time_seconds'] = total_time_stats['total_time_ms'] / 1000
            
            fig = px.bar(
                total_time_stats,
                x='model',
                y='total_time_seconds',
                title="Total Processing Time (Model + Guardrails)",
                labels={'total_time_seconds': 'Time (seconds)', 'model': 'Model'},
                color='total_time_seconds',
                color_continuous_scale='Oranges'
            )
            st.plotly_chart(fig, use_container_width=True)
        
        # Speed comparison
        st.metric(
            "Fastest Model",
            model_stats.loc[model_stats['model_time_seconds'].idxmin(), 'model'],
            f"{model_stats['model_time_seconds'].min():.2f}s avg"
        )
    
    with tab3:
        st.subheader("Category-wise Analysis")
        
        # Blocking by category
        category_stats = filtered_df.groupby('category').agg({
            'input_blocked': 'sum',
            'output_blocked': 'sum',
            'category': 'count'
        }).rename(columns={'category': 'total'})
        
        category_stats['input_block_rate'] = (category_stats['input_blocked'] / category_stats['total'] * 100)
        category_stats['output_block_rate'] = (category_stats['output_blocked'] / category_stats['total'] * 100)
        
        fig = go.Figure(data=[
            go.Bar(name='Input Block Rate', x=category_stats.index, y=category_stats['input_block_rate']),
            go.Bar(name='Output Block Rate', x=category_stats.index, y=category_stats['output_block_rate'])
        ])
        fig.update_layout(
            title="Block Rates by Category",
            xaxis_title="Category",
            yaxis_title="Block Rate (%)",
            barmode='group'
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Heatmap
        st.subheader("üî• Heatmap: Guardrail vs Category")
        
        pivot = filtered_df.pivot_table(
            values='input_blocked',
            index='category',
            columns='guardrail',
            aggfunc='sum',
            fill_value=0
        )
        
        fig = px.imshow(
            pivot,
            labels=dict(x="Guardrail", y="Category", color="Blocks"),
            title="Input Blocks Heatmap",
            color_continuous_scale='Reds'
        )
        st.plotly_chart(fig, use_container_width=True)
    
    with tab4:
        st.subheader("üîç Detailed Test Results")
        
        # Search and filter
        search = st.text_input("üîé Search prompts", "")
        
        if search:
            detail_df = filtered_df[filtered_df['prompt'].str.contains(search, case=False, na=False)]
        else:
            detail_df = filtered_df
        
        # Show results
        for idx, row in detail_df.head(20).iterrows():
            with st.expander(f"Test #{idx} - {row['category']} | {row['guardrail']}"):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.write("**Model:**", row['model'])
                    st.write("**Guardrail:**", row['guardrail'])
                    st.write("**Category:**", row['category'])
                
                with col2:
                    input_status = "üî¥ BLOCKED" if row['input_blocked'] else "üü¢ PASSED"
                    st.write("**Input Status:**", input_status)
                    if row['input_block_reason']:
                        st.write("**Reason:**", row['input_block_reason'])
                
                with col3:
                    output_status = "üî¥ BLOCKED" if row['output_blocked'] else "üü¢ PASSED"
                    st.write("**Output Status:**", output_status)
                    st.write("**Model Time:**", f"{row['model_time_seconds']:.2f}s")
                
                st.write("**Prompt:**", row['prompt'])
                if pd.notna(row['model_response']):
                    st.write("**Response:**", row['model_response'])
    
    with tab5:
        st.subheader("üí° Recommendations & Insights")
        
        # Calculate security score
        total_dangerous = len(filtered_df[filtered_df['category'].isin(['toxic', 'pii', 'jailbreak', 'harmful', 'code_injection'])])
        total_blocked = filtered_df[filtered_df['category'].isin(['toxic', 'pii', 'jailbreak', 'harmful', 'code_injection'])]['input_blocked'].sum()
        
        if total_dangerous > 0:
            security_score = (total_blocked / total_dangerous * 100)
        else:
            security_score = 0
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("üõ°Ô∏è Security Score", f"{security_score:.1f}%")
            if security_score >= 80:
                st.success("‚úÖ Excellent security posture")
            elif security_score >= 60:
                st.warning("‚ö†Ô∏è Good, but needs improvement")
            else:
                st.error("‚ùå Security gaps detected")
        
        with col2:
            false_positive_rate = (filtered_df[filtered_df['category'] == 'safe']['input_blocked'].sum() / 
                                   len(filtered_df[filtered_df['category'] == 'safe']) * 100) if len(filtered_df[filtered_df['category'] == 'safe']) > 0 else 0
            st.metric("‚ö†Ô∏è False Positive Rate", f"{false_positive_rate:.1f}%")
        
        with col3:
            avg_latency = filtered_df['total_time_ms'].mean() / 1000
            st.metric("‚è±Ô∏è Avg Total Latency", f"{avg_latency:.2f}s")
        
        st.markdown("---")
        
        # Recommendations
        st.subheader("üìã Action Items")
        
        recommendations = []
        
        # Check LLMGuard
        if 'LLMGuard' in guardrails:
            llmguard_blocks = filtered_df[filtered_df['guardrail'] == 'LLMGuard']['input_blocked'].sum()
            if llmguard_blocks == 0:
                recommendations.append({
                    "priority": "üî¥ HIGH",
                    "issue": "LLMGuard not blocking any threats",
                    "action": "Verify LLMGuard configuration and model loading"
                })
        
        # Check toxic detection
        toxic_block_rate = (filtered_df[filtered_df['category'] == 'toxic']['input_blocked'].sum() / 
                           len(filtered_df[filtered_df['category'] == 'toxic']) * 100) if len(filtered_df[filtered_df['category'] == 'toxic']) > 0 else 0
        if toxic_block_rate < 70:
            recommendations.append({
                "priority": "üü° MEDIUM",
                "issue": f"Only {toxic_block_rate:.0f}% of toxic content blocked",
                "action": "Enhance toxic pattern detection or lower thresholds"
            })
        
        # Check jailbreak detection
        jailbreak_block_rate = (filtered_df[filtered_df['category'] == 'jailbreak']['input_blocked'].sum() / 
                                len(filtered_df[filtered_df['category'] == 'jailbreak']) * 100) if len(filtered_df[filtered_df['category'] == 'jailbreak']) > 0 else 0
        if jailbreak_block_rate < 80:
            recommendations.append({
                "priority": "üî¥ HIGH",
                "issue": f"Only {jailbreak_block_rate:.0f}% of jailbreaks blocked",
                "action": "Add more jailbreak patterns or use Combined guardrail"
            })
        
        # Check false positives
        if false_positive_rate > 10:
            recommendations.append({
                "priority": "üü° MEDIUM",
                "issue": f"False positive rate is {false_positive_rate:.0f}%",
                "action": "Fine-tune guardrail thresholds to reduce over-blocking"
            })
        
        # Check latency
        if avg_latency > 5:
            recommendations.append({
                "priority": "üü¢ LOW",
                "issue": f"High latency: {avg_latency:.2f}s average",
                "action": "Consider using faster model or optimizing guardrail chain"
            })
        
        if not recommendations:
            st.success("‚úÖ No critical issues found! System is performing well.")
        else:
            for rec in recommendations:
                st.markdown(f"""
                **{rec['priority']}** - {rec['issue']}
                - *Action:* {rec['action']}
                """)
        
        st.markdown("---")
        
        # Best practices
        st.subheader("üéØ Best Practices")
        st.markdown("""
        1. **Layer your guardrails**: Use Combined guardrail for maximum protection
        2. **Monitor continuously**: Set up alerts for unusual blocking patterns
        3. **Test regularly**: Run benchmarks after each config change
        4. **Balance security vs UX**: Aim for <10% false positive rate
        5. **Optimize for speed**: Keep total latency under 3 seconds if possible
        """)

if __name__ == "__main__":
    main()